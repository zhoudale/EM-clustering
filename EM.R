library(car)
library(MASS)


data1 = as.matrix(read.table("data/dataset1.txt"))
data2 = as.matrix(read.table("data/dataset2.txt"))
data3 = as.matrix(read.table("data/dataset3.txt"))


# K-means
kmeans.kernel = function(data, k){
  # data : data to be clusterd
  # k : num of clusters
  num = 0
  sse =0
  nlen = nrow(data)
  newcenter = data[sample(nlen, k), ]
  center = 0 * newcenter
  while((num<1000) & !(identical(center, newcenter))){
    center = newcenter; num = num + 1
    distance = apply(data, 1, function(x) apply(center, 1, function(y) sum((x-y)^2)))
    
    cluster = apply(distance, 2, which.min)
    for (j in 1:k){
      newcenter[j,] = colMeans(data[(cluster==j),])
    }
    sse[num] = sum(sapply(1:nlen, function(x) distance[cluster[x],x])^2)
  }
  plot(1:num,sse,'l')
  dataEllipse(data,groups=factor(cluster), levels=c(0.001))
  a = 0
  for (num in 1:nlen){
    a = a + distance[cluster[num], num]
  }
  return(list("distance" = a, "c" = cluster, "center" = center))
}

kmeans = function(data, k, r){
  # data : data to be clusterd
  # k : num of clusters
  # r : num of different randomly chosen initializations
  dis = -1; 
  cluster = 0;
  center = 0;
  for (i in 1:r){
    res = kmeans.kernel(data, k)
    if((dis == -1) | (res$distance < dis)){
      dis = res$distance
      cluster = res$c
      center = res$center
    }
  }

return(list("center" = center, "cluster" = cluster))
}



###############################
# EM

EM = function(data, k, epsilon, niterations){
#   #################
#   INPUTS
#   %  data: N x d real-valued data matrix
#   %  K: number of clusters (mixture components)
#   %  initialization_method: 1 for memberships, 2 for parameters, 3 for kmeans 
#   %  epsilon: convergence threshold used to detect convergence
#   %  niterations (optional): maximum number of iterations to perform (default 500)
#   %  plotflag (optional): equals 1 to plot parameters during learning, 
#   %                       0 for no plotting (default is 0)
#   %  RSEED (optional): initial seed value for the random number generator
#   %  
#   %
#   % OUTPUTS
#   %  gparams:  2d structure array containing the learned mixture model parameters: 
#    %           gparams(k).weight = weight of component k
#   %           gparams(k).mean = d-dimensional mean vector for kth component 
#   %           gparams(k).covariance = d x d covariance vector for kth component
#   %  memberships: N x K matrix of probability memberships for "data"
#   
#   %  Note: Interpretation of gparams and memberships:
#    %    - gparams(k).weight is the probability that a randomly selected row
#   %         belongs to component (or cluster) i (so it is "cluster size")
#   %    - memberships(i,k) = p(cluster k | x) which is the probability
#   %         (computed via Bayes rule) that vector x was generated by cluster
#   %         k, according to the "generative" probabilistic model. 
##########
# initialize
init = kmeans(data, k, 1)
center = init$center;
#center = data[sample(nlen, k), ]
cov1 = cov(data)
cov = replicate(k, cov1, simplify =F)
alpha = rep(1/k, k)
ll = 0; e = 1; num = 1
# e-m steps
while((abs(e) > epsilon) & (num < niterations)){
  
  mem = m.step(data, center, cov, alpha, k)
  membership = mem$membership
  res = e.step(data, membership, k)
  
  cluster = res$cluster
  center = res$center
  cov = res$cov
  alpha = res$alpha
  ll[num+1] = mem$ll
  e = 10000 * (ll[num] - ll[num+1])/ll[num]
  num = num+1
  
}
plot(2:(num-1), ll[2:(num-1)], 'l', xlab = '# of iterations', ylab = 'log-likelihood')
dataEllipse(data,groups=factor(cluster), levels=c(0.95))
return(ll)
}

# e-step
e.step = function(data, membership, k){
  cluster = apply(membership, 1, which.max)
  center = sapply(1:k, function(i) colMeans(data[cluster==i,]))
  center = t(center)
  cov  = lapply(1:k, function(i) cov(data[cluster==i,]))
  alpha = colMeans(membership)
  res = list("cluster" = cluster, "center" = center, "alpha" = alpha, "cov" = cov)
  return(res)
}

# m-step
m.step = function(data, center, cov, alpha, k){
  inv.cov = lapply(cov,function(x) ginv(x))
  # membership1 = apply(data, 1, function(x) apply(center, 1, function(y) sapply(inv.cov, function(z) (x-y)%*%z%*%(x-y))))
  membership1 = apply(data, 1, function(x) sapply(1:k, function(i) (x - center[i,])%*%inv.cov[[i]]%*%(x - center[i,])))
  det.cov = sapply(cov, det)
  membership = apply(membership1, 2, function(x)  exp(-x/2)/sqrt(det.cov))
  membership = apply(membership, 2, function(x) x*alpha)
  ll = sum(apply(membership, 2, function(x) log(sum(x))))
  membership = apply(membership, 2, function(x) x/sum(x))
  membership = t(membership)
  mem = list("membership" =membership, "ll" = ll)
  return(mem)
}

nEM = function(data, n, epsilon, niterations){
  d = ncol(data)
  l = nrow(data)
  logLike = 0
  bic = 0
  logN = log(d * l)/2
  p = (1 + d + d*(d+1)/2) 
  for (i in 2:n){
    ll = EM(data, i, epsilon, niterations)
    ll = max(ll[-1])
    logLike[i] = ll
    bic[i] = ll - (i*p - 1) * logN 
  }
  # compute n = 1, explained by a single guassian
  cov = cov(data)
  inv.cov = ginv(cov)
  dcov = det(cov)
  mu = colMeans(data)
  dis = apply(data, 1, "-", mu)
  ll = sum(log(apply(dis, 2, function(x) exp(-x%*%inv.cov%*%x/2)/sqrt(dcov))))
  logLike[1] = ll
  bic[1] = ll - (p - 1) * logN 
  plot(1:n, bic)
  plot(1:n, logLike)
  return(list("bic"= bic,"ll"=logLike))
}


